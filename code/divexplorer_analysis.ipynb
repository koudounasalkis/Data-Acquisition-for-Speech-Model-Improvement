{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db905f0a",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "powered-information",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T15:20:44.908897Z",
     "start_time": "2022-04-20T15:20:44.883774Z"
    },
    "id": "powered-information"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-doctor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T15:20:45.349495Z",
     "start_time": "2022-04-20T15:20:44.910825Z"
    },
    "id": "built-doctor"
   },
   "outputs": [],
   "source": [
    "import divexplorer \n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n",
    "import os\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from utils_analysis import plotMultipleSV, plotShapleyValue\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c80058f0",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26247e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for sorting data cohorts\n",
    "def sortItemset(x, abbreviations={}):\n",
    "    x = list(x)\n",
    "    x.sort()\n",
    "    x = \", \".join(x)\n",
    "    for k, v in abbreviations.items():\n",
    "        x = x.replace(k, v)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e539fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_in_itemset(itemset, attributes, alls = True):\n",
    "    \"\"\" Check if attributes are in the itemset (all or at least one)\n",
    "    \n",
    "    Args:\n",
    "        itemset (frozenset): the itemset\n",
    "        attributes (list): list of itemset of interest\n",
    "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
    "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Avoid returning the empty itemset (i.e., info of entire dataset)\n",
    "    if itemset == frozenset() and attributes:\n",
    "        return False\n",
    "    \n",
    "    for item in itemset:\n",
    "        # Get the attribute\n",
    "        attr_i = item.split(\"=\")[0]\n",
    "        \n",
    "        #If True, check if ALL attributes of the itemset are the input attributes.\n",
    "        if alls:\n",
    "            # Check if the attribute is present. If not, the itemset is not admitted\n",
    "            if attr_i not in attributes:\n",
    "                return False\n",
    "        else:\n",
    "            # Check if least one attribute. If yes, return True\n",
    "            if attr_i in attributes:\n",
    "                return True\n",
    "    if alls:\n",
    "        # All attributes of the itemset are indeed admitted\n",
    "        return True\n",
    "    else:\n",
    "        # Otherwise, it means that we find None\n",
    "        return False\n",
    "    \n",
    "def filter_itemset_df_by_attributes(df: pd.DataFrame, attributes: list, alls = True, itemset_col_name: str = \"itemsets\") -> pd.DataFrame:\n",
    "    \"\"\"Get the set of itemsets that have the attributes in the input list (all or at least one)\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the input itemsets (with their info). \n",
    "        attributes (list): list of itemset of interest\n",
    "        alls (bool): If True, check if ALL attributes of the itemset are the input attributes. \n",
    "        If False, check AT LEAST one attribute of the itemset is in the input attributes.\n",
    "        itemset_col_name (str) : the name of the itemset column, \"itemsets\" as default\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: the set of itemsets (with their info)\n",
    "    \"\"\"\n",
    "\n",
    "    return df.loc[df[itemset_col_name].apply(lambda x: attributes_in_itemset(x, attributes, alls = alls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d02de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define abbreviations for plot and visualization\n",
    "from divexplorer.FP_Divergence import abbreviateDict\n",
    "abbreviations = {'Self-reported fluency level=native': 'fluency=native', \\\n",
    "                  'total_silence':'tot_silence', 'location': 'loc', \\\n",
    "                  'Current language used for work/school=English (United States)': 'lang=EN_US', \\\n",
    "                  'ageRange': 'age', \\\n",
    "                  'speakerId' : 'spkID', \\\n",
    "                  'First Language spoken=English (United States)':  'lang=EN_US', \\\n",
    "                  'trimmed': 'trim', \\\n",
    "                  'total_': 'tot_', \\\n",
    "                  'speed_rate_word':'speakRate', \\\n",
    "                  'speed_rate_char':'speakCharRate', \\\n",
    "                  'change language': 'change lang', \\\n",
    "                  'duration': 'dur'}\n",
    "\n",
    "abbreviations_shorter = abbreviations.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "occupational-madrid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-20T15:07:23.652910Z",
     "start_time": "2022-04-20T15:07:23.612488Z"
    },
    "id": "occupational-madrid"
   },
   "source": [
    "# Define targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461193a2",
   "metadata": {
    "id": "461193a2"
   },
   "outputs": [],
   "source": [
    "## Target for DivExplorer: \n",
    "# 'prediction' is 1 if predicted_intet == original_intent, 0 otherwise\n",
    "target_col = 'prediction' \n",
    "target_metric = 'd_posr'\n",
    "target_div = 'd_accuracy'\n",
    "t_value_col = 't_value_tp_fn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8efa3b44",
   "metadata": {
    "id": "8efa3b44"
   },
   "outputs": [],
   "source": [
    "## Columns for visualization\n",
    "show_cols = ['support', 'itemsets', '#errors', '#corrects', 'accuracy', \\\n",
    "                'd_accuracy', 't_value', 'support_count', 'length']\n",
    "remapped_cols = {'tn': '#errors', 'tp': '#corrects', 'posr': 'accuracy', \\\n",
    "                target_metric: target_div, 't_value_tp_fn': 't_value'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bbd447",
   "metadata": {},
   "source": [
    "# FSC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e257bab",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73fbc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
    "from divexplorer.FP_Divergence import FP_Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b043ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns of the df file that we are going to analyze \n",
    "demo_cols = ['Self-reported fluency level ', 'First Language spoken',\n",
    "       'Current language used for work/school', 'gender', 'ageRange']\n",
    "\n",
    "slot_cols = ['action', 'object', 'location']\n",
    "\n",
    "signal_cols = ['total_silence', 'total_duration', 'trimmed_duration', \n",
    "       'n_words', 'speed_rate_word', 'speed_rate_word_trimmed'] \n",
    "\n",
    "input_cols = demo_cols + signal_cols + slot_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a64f09",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = \"divexplorer\" # \"divexplorer\" or \"clustering\"\n",
    "\n",
    "## Define the minimum support threshold for data subgroups\n",
    "if approach == \"divexplorer\":\n",
    "    min_sup = 0.03\n",
    "elif approach == \"clustering\":\n",
    "    min_sup = 0.000001\n",
    "    num_clusters = 20\n",
    "\n",
    " \n",
    "configs = [\n",
    "    \"fsc_original\", \n",
    "    ] \n",
    "    \n",
    "FP_fm_dict = {}\n",
    "fp_divergence_dict = {}\n",
    "df_dict = {}\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    input_file_divexplorer = os.path.join(\\\n",
    "        os.getcwd(), config, \"0\", \"predictions_5.csv\")\n",
    "\n",
    "    df = pd.read_csv(input_file_divexplorer, index_col=0)\n",
    "\n",
    "    ## Add SpeakerID information if it is present in the df\n",
    "    if \"speakerId\" in input_cols:\n",
    "        df['speakerId'] = df.index.map(lambda x: x.split(\"/\")[2])\n",
    "\n",
    "    if approach == 'divexplorer':\n",
    "\n",
    "        ## Discretize the dataframe\n",
    "        from divergence_utils import discretize\n",
    "\n",
    "        df_discretized = discretize(\n",
    "            df[input_cols+[target_col]],\n",
    "            bins=3,\n",
    "            attributes=input_cols,\n",
    "            strategy=\"quantile\", \n",
    "            round_v = 2,\n",
    "            min_distinct=5,\n",
    "        )\n",
    "\n",
    "        ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "        replace_values = {}\n",
    "\n",
    "        for i in range(0,len(signal_cols)):\n",
    "\n",
    "            for v in df_discretized[signal_cols[i]].unique():\n",
    "                if \"<=\" == v[0:2]:\n",
    "                    replace_values[v] = \"low\"\n",
    "                elif \">\" == v[0]:\n",
    "                    replace_values[v] = \"high\"\n",
    "                elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                    replace_values[v] = \"medium\"\n",
    "                else:\n",
    "                    raise ValueError(v)\n",
    "\n",
    "            df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "    \n",
    "    elif approach == 'clustering':\n",
    "\n",
    "        df_discretized = df[[f'speech_cluster_id_{k}' for k in [num_clusters]] + [target_col]]\n",
    "    \n",
    "    ## Create dict of Divergence df\n",
    "    df_dict[config] = df_discretized\n",
    "    fp_diver = FP_DivergenceExplorer(\n",
    "        df_discretized, \n",
    "        true_class_name=target_col, \n",
    "        class_map={\"P\":1, \"N\":0}\n",
    "        )\n",
    "    FP_fm = fp_diver.getFrequentPatternDivergence(\n",
    "        min_support=min_sup, \n",
    "        metrics=[target_metric]\n",
    "        )\n",
    "    FP_fm.rename(\n",
    "        columns=remapped_cols, \n",
    "        inplace=True\n",
    "        )\n",
    "    FP_fm = FP_fm[show_cols].copy()\n",
    "    FP_fm['accuracy'] = round(FP_fm['accuracy'], 5)\n",
    "    FP_fm['d_accuracy'] = round(FP_fm['d_accuracy'], 5)\n",
    "    FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
    "    FP_fm_dict[config] = FP_fm\n",
    "    fp_divergence_dict[config] = FP_Divergence(FP_fm, target_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a92bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discretize the dataframe\n",
    "from divergence_utils import discretize\n",
    "\n",
    "df_train_rest = pd.read_csv(\"data/fsc/train_data_20.csv\")\n",
    "\n",
    "if approach == 'divexplorer':\n",
    "    df_discretized_rest = discretize(\n",
    "        df_train_rest[input_cols],\n",
    "        bins=3,\n",
    "        attributes=input_cols,\n",
    "        strategy=\"quantile\", \n",
    "        round_v = 2,\n",
    "        min_distinct=5,\n",
    "    )\n",
    "\n",
    "    ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "    replace_values = {}\n",
    "\n",
    "    for i in range(0,len(signal_cols)):\n",
    "\n",
    "        for v in df_discretized_rest[signal_cols[i]].unique():\n",
    "            if \"<=\" == v[0:2]:\n",
    "                replace_values[v] = \"low\"\n",
    "            elif \">\" == v[0]:\n",
    "                replace_values[v] = \"high\"\n",
    "            elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                replace_values[v] = \"medium\"\n",
    "            else:\n",
    "                raise ValueError(v)\n",
    "\n",
    "        df_discretized_rest[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "\n",
    "elif approach == 'clustering':\n",
    "    \n",
    "    df_discretized_rest = df_train_rest[[f'speech_cluster_id_{k}' for k in [num_clusters]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac01c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "RANDOM = False\n",
    "th_redundancy = 0.15\n",
    "NUM_SUBGROUPS = [2,3,4,5]\n",
    "\n",
    "if RANDOM:\n",
    "\n",
    "    # for num_samples in [176, 463, 388, 599, 547, 829, 600, 1027]:\n",
    "    for num_samples in [226, 406, 382, 874, 422, 1046, 509, 1276]:\n",
    "\n",
    "        df_train_rest = pd.read_csv(\"data/fsc/train_data_20.csv\")\n",
    "        df_train_rest = df_train_rest.sample(frac=1).reset_index(drop=True)\n",
    "        df_train_rest = df_train_rest.head(num_samples)\n",
    "        print(\"Total number of samples in to be added: \", len(df_train_rest))\n",
    "\n",
    "        df_train = pd.read_csv(\"data/fsc/train_data_80.csv\")\n",
    "        df_train = df_train.append(df_train_rest, ignore_index=True)\n",
    "        df_train.to_csv(f\"data/fsc/new_data/train_data_random_{num_samples}.csv\", index=False)\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "else: \n",
    "\n",
    "    for NS in NUM_SUBGROUPS:\n",
    "\n",
    "        print(\"Number of problematic subgroups: \", NS)\n",
    "\n",
    "        fp_divergence_i = fp_divergence_dict[config]\n",
    "        FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "        pr_bot = FPdiv.head(NS).copy()\n",
    "        itemsets = []\n",
    "        for i in range(NS):\n",
    "            if approach == 'divexplorer':\n",
    "                itemsets.append(list(pr_bot.itemsets.values[i]))\n",
    "            elif approach == 'clustering':\n",
    "                itemsets.append(list(pr_bot.itemsets.values[i])[0])\n",
    "\n",
    "        ## Create a column in the df, and assign a class to each sample:\n",
    "        # - 1 if the sample is in the most divergent itemset\n",
    "        # - 2 if the sample is in the second most divergent itemset\n",
    "        # - 3 if the sample is in the third most divergent itemset\n",
    "        # - ...\n",
    "        # - 0 otherwise\n",
    "        df_discretized_rest[\"subgID\"] = 0\n",
    "\n",
    "        if approach == 'divexplorer':\n",
    "            for i in range(0, len(df_discretized_rest)):\n",
    "                for value,itemset in enumerate(itemsets):\n",
    "                    ks = []\n",
    "                    vs = []\n",
    "                    for item in itemset:\n",
    "                        k, v = item.split(\"=\")\n",
    "                        ks.append(k)\n",
    "                        vs.append(v)\n",
    "                    if all(df_discretized_rest.loc[i, ks] == vs):\n",
    "                        if df_discretized_rest.loc[i, \"subgID\"] == 0:\n",
    "                            df_discretized_rest.loc[i, \"subgID\"] = value+1\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "        elif approach == 'clustering':\n",
    "            for i in range(0, len(df_discretized_rest)):\n",
    "                for value,itemset in enumerate(itemsets):\n",
    "                    k, v = itemset.split(\"=\")\n",
    "                    if df_discretized_rest.loc[i, k] == int(v):\n",
    "                        if df_discretized_rest.loc[i, \"subgID\"] == 0:\n",
    "                            df_discretized_rest.loc[i, \"subgID\"] = value+1\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "        ## Keep in df_discretized_rest only the elements with subgID != 0\n",
    "        df_train_rest = pd.read_csv(\"data/fsc/train_data_20.csv\")\n",
    "        df_train_rest = df_train_rest.loc[df_discretized_rest[\"subgID\"]!=0]\n",
    "        print(\"Total number of samples in to be added: \", len(df_train_rest))\n",
    "        \n",
    "        ## Append df_discretized_rest to df_train\n",
    "        df_train = pd.read_csv(\"data/fsc/train_data_80.csv\")\n",
    "        df_train = df_train.append(df_train_rest, ignore_index=True)\n",
    "        df_train.to_csv(f\"data/fsc/new_data/train_data_{approach}_k{NS}.csv\", index=False)\n",
    "        print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baaa020",
   "metadata": {},
   "source": [
    "## Divergence wav2vec 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabbf26",
   "metadata": {},
   "source": [
    "### Retrieve Data and Compute Divergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a945ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.03\n",
    "\n",
    "configs = [\n",
    "    \"fsc_original\", \n",
    "    \"fsc_divexplorer_test-k_2\",\n",
    "    \"fsc_divexplorer_test-k_3\",\n",
    "    \"fsc_divexplorer_test-k_4\",\n",
    "    \"fsc_divexplorer_test-k_5\",\n",
    "    \"fsc_clustering_test-k_2\",\n",
    "    \"fsc_clustering_test-k_3\",\n",
    "    \"fsc_clustering_test-k_4\",\n",
    "    \"fsc_clustering_test-k_5\",\n",
    "    \"fsc_random_test-226\",\n",
    "    \"fsc_random_test-382\",\n",
    "    \"fsc_random_test-406\",\n",
    "    \"fsc_random_test-422\",\n",
    "    \"fsc_random_test-509\",\n",
    "    \"fsc_random_test-874\",\n",
    "    \"fsc_random_test-1046\",\n",
    "    \"fsc_random_test-1276\"\n",
    "    ] \n",
    "    \n",
    "FP_fm_dict = {}\n",
    "fp_divergence_dict = {}\n",
    "df_dict = {}\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    if \"divexplorer\" in config or \"clustering\" in config or \"random\" in config:\n",
    "        folders = config.split(\"-\")\n",
    "        input_file_divexplorer = os.path.join(\\\n",
    "            os.getcwd(), folders[0], folders[1], \"0\", \"df_test.csv\")\n",
    "    else:\n",
    "        input_file_divexplorer = os.path.join(\\\n",
    "            os.getcwd(), config, \"0\", \"df_test.csv\")\n",
    "\n",
    "    df = pd.read_csv(input_file_divexplorer, index_col=0)\n",
    "\n",
    "    ## Add SpeakerID information if it is present in the df\n",
    "    if \"speakerId\" in input_cols:\n",
    "        df['speakerId'] = df.index.map(lambda x: x.split(\"/\")[2])\n",
    "\n",
    "    ## Discretize the dataframe\n",
    "    from divergence_utils import discretize\n",
    "\n",
    "    df_discretized = discretize(\n",
    "        df[input_cols+[target_col]],\n",
    "        bins=3,\n",
    "        attributes=input_cols,\n",
    "        strategy=\"quantile\", \n",
    "        round_v = 2,\n",
    "        min_distinct=5,\n",
    "    )\n",
    "\n",
    "    ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "    replace_values = {}\n",
    "\n",
    "    for i in range(0,len(signal_cols)):\n",
    "\n",
    "        for v in df_discretized[signal_cols[i]].unique():\n",
    "            if \"<=\" == v[0:2]:\n",
    "                replace_values[v] = \"low\"\n",
    "            elif \">\" == v[0]:\n",
    "                replace_values[v] = \"high\"\n",
    "            elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                replace_values[v] = \"medium\"\n",
    "            else:\n",
    "                raise ValueError(v)\n",
    "\n",
    "        df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "            \n",
    "    ## Create dict of Divergence df\n",
    "    df_dict[config] = df_discretized\n",
    "    fp_diver = FP_DivergenceExplorer(\n",
    "        df_discretized, \n",
    "        true_class_name=target_col, \n",
    "        class_map={\"P\":1, \"N\":0}\n",
    "        )\n",
    "    FP_fm = fp_diver.getFrequentPatternDivergence(\n",
    "        min_support=min_sup, \n",
    "        metrics=[target_metric]\n",
    "        )\n",
    "    FP_fm.rename(\n",
    "        columns=remapped_cols, \n",
    "        inplace=True\n",
    "        )\n",
    "    FP_fm = FP_fm[show_cols].copy()\n",
    "    FP_fm['accuracy'] = round(FP_fm['accuracy'], 5)\n",
    "    FP_fm['d_accuracy'] = round(FP_fm['d_accuracy'], 5)\n",
    "    FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
    "    FP_fm_dict[config] = FP_fm\n",
    "    fp_divergence_dict[config] = FP_Divergence(FP_fm, target_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805595c",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for wav2vec 2.0 base\n",
    "config = 'fsc_original'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "th_redundancy = None \n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)\n",
    "\n",
    "## Compute the mean negative divergence for wav2vec 2.0 base\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_accuracy'] <= 0]))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(10).copy()\n",
    "print(\"Mean negative divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(20).copy()\n",
    "print(\"Mean negative divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(50).copy()\n",
    "print(\"Mean negative divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean negative divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv.copy()\n",
    "print(\"\\nMean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdce4de",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for wav2vec 2.0 base rebalanced w/ random boosting \n",
    "config = 'fsc_random_test-874'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "th_redundancy = None \n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)\n",
    "\n",
    "## Compute the mean negative divergence for wav2vec 2.0 base rebalanced w/ random boosting \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_accuracy'] <= 0]))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(10).copy()\n",
    "print(\"Mean negative divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(20).copy()\n",
    "print(\"Mean negative divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(50).copy()\n",
    "print(\"Mean negative divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean negative divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv.copy()\n",
    "print(\"\\nMean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c80186",
   "metadata": {},
   "source": [
    "### DivExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f815e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for wav2vec 2.0 base rebalanced w/ DivExplorer \n",
    "config = 'fsc_divexplorer_test-k_2'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "th_redundancy = None \n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)\n",
    "\n",
    "## Compute the mean negative divergence for wav2vec 2.0 base rebalanced w/ DivExplorer \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(10).copy()\n",
    "print(\"Mean negative divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(20).copy()\n",
    "print(\"Mean negative divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(50).copy()\n",
    "print(\"Mean negative divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean negative divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv.copy()\n",
    "print(\"\\nMean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "print(\"Total subgroups: \", len(FPdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a441a",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bcd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for wav2vec 2.0 base rebalanced w/ Clustering \n",
    "config = 'fsc_clustering_test-k_2'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "th_redundancy = None\n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)\n",
    "\n",
    "## Compute the mean negative divergence for wav2vec 2.0 base rebalanced w/ DivExplorer \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(10).copy()\n",
    "print(\"Mean negative divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(20).copy()\n",
    "print(\"Mean negative divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(50).copy()\n",
    "print(\"Mean negative divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean negative divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv.copy()\n",
    "print(\"\\nMean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "print(\"Total subgroups: \", len(FPdiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e7d20f",
   "metadata": {},
   "source": [
    "# ITALIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef4184",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4bc5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from divexplorer.FP_DivergenceExplorer import FP_DivergenceExplorer\n",
    "from divexplorer.FP_Divergence import FP_Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4f3c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns of the df file that we are going to analyze \n",
    "demo_cols = ['gender', 'age', 'region', 'nationality', 'lisp', 'education']\n",
    "\n",
    "slot_cols = ['action', 'scenario']\n",
    "\n",
    "rec_set_cols = ['environment', 'device', 'field']\n",
    "\n",
    "signal_cols = ['total_silence', 'total_duration', 'trimmed_duration', \n",
    "'n_words', 'speed_rate_word', 'speed_rate_word_trimmed'] \n",
    "\n",
    "input_cols = demo_cols + slot_cols + rec_set_cols + signal_cols "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c54b7",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = \"divexplorer\" # \"divexplorer\" or \"clustering\"\n",
    "\n",
    "## Define the minimum support threshold for data subgroups\n",
    "if approach == \"divexplorer\":\n",
    "    min_sup = 0.03\n",
    "elif approach == \"clustering\":\n",
    "    min_sup = 0.000001\n",
    "    num_clusters = 10\n",
    "\n",
    "configs = [\n",
    "    \"italic_original\", \n",
    "    ]\n",
    "\n",
    "FP_fm_dict = {}\n",
    "fp_divergence_dict = {}\n",
    "df_dict = {}\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    input_file_divexplorer = os.path.join(\\\n",
    "        os.getcwd(), config, \"0\", \"predictions_7.csv\")\n",
    "\n",
    "    df = pd.read_csv(input_file_divexplorer, index_col=0)\n",
    "    df['action'] = df['intent'].apply(lambda x: x.split(\"_\")[0])\n",
    "    df['scenario'] = df['intent'].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "    if approach == 'divexplorer':\n",
    "\n",
    "        ## Discretize the dataframe\n",
    "        from divergence_utils import discretize\n",
    "\n",
    "        df_discretized = discretize(\n",
    "            df[input_cols+[target_col]],\n",
    "            bins=3,\n",
    "            attributes=input_cols,\n",
    "            strategy=\"quantile\", \n",
    "            round_v = 2,\n",
    "            min_distinct=5,\n",
    "        )\n",
    "\n",
    "        ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "        replace_values = {}\n",
    "\n",
    "        for i in range(0,len(signal_cols)):\n",
    "\n",
    "            for v in df_discretized[signal_cols[i]].unique():\n",
    "                if \"<=\" == v[0:2]:\n",
    "                    replace_values[v] = \"low\"\n",
    "                elif \">\" == v[0]:\n",
    "                    replace_values[v] = \"high\"\n",
    "                elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                    replace_values[v] = \"medium\"\n",
    "                else:\n",
    "                    raise ValueError(v)\n",
    "\n",
    "            df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "\n",
    "    elif approach == 'clustering':\n",
    "\n",
    "        df_discretized = df[[f'speech_cluster_id_{k}' for k in [num_clusters]] + [target_col]]\n",
    "    \n",
    "    ## Create dict of Divergence df\n",
    "    df_dict[config] = df_discretized\n",
    "    fp_diver = FP_DivergenceExplorer(\n",
    "        df_discretized, \n",
    "        true_class_name=target_col, \n",
    "        class_map={\"P\":1, \"N\":0}\n",
    "        )\n",
    "    FP_fm = fp_diver.getFrequentPatternDivergence(\n",
    "        min_support=min_sup, \n",
    "        metrics=[target_metric]\n",
    "        )\n",
    "    FP_fm.rename(\n",
    "        columns=remapped_cols, \n",
    "        inplace=True\n",
    "        )\n",
    "    FP_fm = FP_fm[show_cols].copy()\n",
    "    FP_fm['accuracy'] = round(FP_fm['accuracy'], 5)\n",
    "    FP_fm['d_accuracy'] = round(FP_fm['d_accuracy'], 5)\n",
    "    FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
    "    FP_fm_dict[config] = FP_fm\n",
    "    fp_divergence_dict[config] = FP_Divergence(FP_fm, target_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a994da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discretize the dataframe\n",
    "from divergence_utils import discretize\n",
    "\n",
    "df_train_rest = pd.read_csv(\"data/italic/train_data_20.csv\")\n",
    "df_train_rest['action'] = df_train_rest['intent'].apply(lambda x: x.split(\"_\")[0])\n",
    "df_train_rest['scenario'] = df_train_rest['intent'].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "if approach == 'divexplorer':\n",
    "    df_discretized_rest = discretize(\n",
    "        df_train_rest[input_cols],\n",
    "        bins=3,\n",
    "        attributes=input_cols,\n",
    "        strategy=\"quantile\", \n",
    "        round_v = 2,\n",
    "        min_distinct=5,\n",
    "    )\n",
    "\n",
    "    ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "    replace_values = {}\n",
    "\n",
    "    for i in range(0,len(signal_cols)):\n",
    "\n",
    "        for v in df_discretized_rest[signal_cols[i]].unique():\n",
    "            if \"<=\" == v[0:2]:\n",
    "                replace_values[v] = \"low\"\n",
    "            elif \">\" == v[0]:\n",
    "                replace_values[v] = \"high\"\n",
    "            elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                replace_values[v] = \"medium\"\n",
    "            else:\n",
    "                raise ValueError(v)\n",
    "\n",
    "        df_discretized_rest[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "\n",
    "elif approach == 'clustering':\n",
    "    \n",
    "    df_discretized_rest = df_train_rest[[f'speech_cluster_id_{k}' for k in [num_clusters]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aced52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "RANDOM = False\n",
    "NUM_SUBGROUPS = [2,3,4,5]\n",
    "th_redundancy = 0.04\n",
    "\n",
    "if RANDOM:\n",
    "\n",
    "    for num_samples in [154, 252, 383, 540, 548, 604, 945, 1035]:\n",
    "\n",
    "        df_train_rest = pd.read_csv(\"data/italic/train_data_20.csv\")\n",
    "        df_train_rest = df_train_rest.sample(frac=1).reset_index(drop=True)\n",
    "        df_train_rest = df_train_rest.head(num_samples)\n",
    "        print(\"Total number of samples in to be added: \", len(df_train_rest))\n",
    "\n",
    "        df_train = pd.read_csv(\"data/italic/train_data_80.csv\")\n",
    "        df_train = df_train.append(df_train_rest, ignore_index=True)\n",
    "        df_train.to_csv(f\"data/italic/new_data/train_data_random_k{num_samples}.csv\", index=False)\n",
    "        print(\"----------------------------------\")\n",
    "\n",
    "else: \n",
    "\n",
    "    for NS in NUM_SUBGROUPS:\n",
    "\n",
    "        print(\"Number of problematic subgroups: \", NS)\n",
    "\n",
    "        fp_divergence_i = fp_divergence_dict[config]\n",
    "        FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "        pr_bot = FPdiv.head(NS).copy()\n",
    "        itemsets = []\n",
    "        for i in range(NS):\n",
    "            if approach == 'divexplorer':\n",
    "                itemsets.append(list(pr_bot.itemsets.values[i]))\n",
    "            elif approach == 'clustering':\n",
    "                itemsets.append(list(pr_bot.itemsets.values[i])[0])\n",
    "\n",
    "        ## Create a column in the df, and assign a class to each sample:\n",
    "        # - 1 if the sample is in the most divergent itemset\n",
    "        # - 2 if the sample is in the second most divergent itemset\n",
    "        # - 3 if the sample is in the third most divergent itemset\n",
    "        # - ...\n",
    "        # - 0 otherwise\n",
    "        df_discretized_rest[\"subgID\"] = 0\n",
    "\n",
    "        if approach == 'divexplorer':\n",
    "            for i in range(0, len(df_discretized_rest)):\n",
    "                for value,itemset in enumerate(itemsets):\n",
    "                    ks = []\n",
    "                    vs = []\n",
    "                    for item in itemset:\n",
    "                        k, v = item.split(\"=\")\n",
    "                        ks.append(k)\n",
    "                        vs.append(v)\n",
    "                    if all(df_discretized_rest.loc[i, ks] == vs):\n",
    "                        if df_discretized_rest.loc[i, \"subgID\"] == 0:\n",
    "                            df_discretized_rest.loc[i, \"subgID\"] = value+1\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "        elif approach == 'clustering':\n",
    "            for i in range(0, len(df_discretized_rest)):\n",
    "                for value,itemset in enumerate(itemsets):\n",
    "                    k, v = itemset.split(\"=\")\n",
    "                    if df_discretized_rest.loc[i, k] == int(v):\n",
    "                        if df_discretized_rest.loc[i, \"subgID\"] == 0:\n",
    "                            df_discretized_rest.loc[i, \"subgID\"] = value+1\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "        ## Keep in df_discretized_rest only the elements with subgID != 0\n",
    "        df_train_rest = pd.read_csv(\"data/italic/train_data_20.csv\")\n",
    "        df_train_rest = df_train_rest.loc[df_discretized_rest[\"subgID\"]!=0]\n",
    "        print(\"Total number of samples in to be added: \", len(df_train_rest))\n",
    "\n",
    "        ## Append df_discretized_rest to df_train\n",
    "        df_train = pd.read_csv(\"data/italic/train_data_80.csv\")\n",
    "        df_train = df_train.append(df_train_rest, ignore_index=True)\n",
    "        df_train.to_csv(f\"data/italic/new_data/train_data_{approach}_k{NS}_{limit}.csv\", index=False)\n",
    "        print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e5725",
   "metadata": {},
   "source": [
    "## Divergence XLSR300m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.03\n",
    "\n",
    "configs = [\n",
    "    \"xlsr_300_original\", \n",
    "    \"xlsr_300_random\",\n",
    "    \"xlsr_300_divexplorer\",\n",
    "    \"xlsr_300_clustering\",\n",
    "    ]\n",
    "\n",
    "FP_fm_dict = {}\n",
    "fp_divergence_dict = {}\n",
    "df_dict = {}\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    if \"divexplorer\" in config or \"clustering\" in config or \"random\" in config:\n",
    "        folders = config.split(\"-\")\n",
    "        input_file_divexplorer = os.path.join(\\\n",
    "            os.getcwd(), folders[0], folders[1], \"0\", \"df_test.csv\")\n",
    "    else:\n",
    "        input_file_divexplorer = os.path.join(\\\n",
    "            os.getcwd(), config, \"0\", \"df_test.csv\")\n",
    "\n",
    "    df = pd.read_csv(input_file_divexplorer, index_col=0)\n",
    "    df['action'] = df['intent'].apply(lambda x: x.split(\"_\")[0])\n",
    "    df['scenario'] = df['intent'].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "    ## Discretize the dataframe\n",
    "    from divergence_utils import discretize\n",
    "\n",
    "    df_discretized = discretize(\n",
    "        df[input_cols+[target_col]],\n",
    "        bins=3,\n",
    "        attributes=input_cols,\n",
    "        strategy=\"quantile\", \n",
    "        round_v = 2,\n",
    "        min_distinct=5,\n",
    "    )\n",
    "\n",
    "    ## Replace values with ranges: \"low\", \"medium\", \"high\"\n",
    "    replace_values = {}\n",
    "\n",
    "    for i in range(0,len(signal_cols)):\n",
    "\n",
    "        for v in df_discretized[signal_cols[i]].unique():\n",
    "            if \"<=\" == v[0:2]:\n",
    "                replace_values[v] = \"low\"\n",
    "            elif \">\" == v[0]:\n",
    "                replace_values[v] = \"high\"\n",
    "            elif \"(\"  == v[0] and \"]\"  == v[-1]:\n",
    "                replace_values[v] = \"medium\"\n",
    "            else:\n",
    "                raise ValueError(v)\n",
    "\n",
    "        df_discretized[signal_cols[i]].replace(replace_values, inplace=True)\n",
    "\n",
    "    ## Create dict of Divergence df\n",
    "    df_dict[config] = df_discretized\n",
    "    fp_diver = FP_DivergenceExplorer(\n",
    "        df_discretized, \n",
    "        true_class_name=target_col, \n",
    "        class_map={\"P\":1, \"N\":0}\n",
    "        )\n",
    "    FP_fm = fp_diver.getFrequentPatternDivergence(\n",
    "        min_support=min_sup, \n",
    "        metrics=[target_metric]\n",
    "        )\n",
    "    FP_fm.rename(\n",
    "        columns=remapped_cols, \n",
    "        inplace=True\n",
    "        )\n",
    "    FP_fm = FP_fm[show_cols].copy()\n",
    "    FP_fm['accuracy'] = round(FP_fm['accuracy'], 5)\n",
    "    FP_fm['d_accuracy'] = round(FP_fm['d_accuracy'], 5)\n",
    "    FP_fm['t_value'] = round(FP_fm['t_value'], 2)\n",
    "    FP_fm_dict[config] = FP_fm\n",
    "    fp_divergence_dict[config] = FP_Divergence(FP_fm, target_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98110593",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for XLSR300m\n",
    "config = 'xlsr300_original'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "th_redundancy = None\n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)\n",
    "\n",
    "## Compute the mean divergence for XLSR300m\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_accuracy'] <= 0]))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(10).copy()\n",
    "print(\"Mean divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(20).copy()\n",
    "print(\"Mean divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(50).copy()\n",
    "print(\"Mean divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv.copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ebb111",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e50cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for XLSR300m rebalanced with Random Boosting\n",
    "config = 'xlsr_300_random'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "th_redundancy = None\n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)\n",
    "\n",
    "## Compute the mean divergence for XLSR300m\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_accuracy'] <= 0]))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(5).copy()\n",
    "print(\"Mean divergence top 5:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(10).copy()\n",
    "print(\"Mean divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(20).copy()\n",
    "print(\"Mean divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(50).copy()\n",
    "print(\"Mean divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "\n",
    "## Retrieve Most Positively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy) \n",
    "pr_top = FPdiv.head(n).copy()\n",
    "pr_top[\"support\"] = pr_top[\"support\"].round(2)\n",
    "pr_top[\"#errors\"] = pr_top[\"#errors\"].astype(int)\n",
    "pr_top[\"#corrects\"] = pr_top[\"#corrects\"].astype(int)\n",
    "pr_top[\"accuracy\"] = (pr_top[\"accuracy\"]*100).round(3)\n",
    "pr_top[\"d_accuracy\"] = (pr_top[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_top = pr_top[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_top['itemsets'] = pr_l_top['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_top)\n",
    "\n",
    "## Compute the mean divergence for XLSR300m\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)\n",
    "print(\"Total positive subgroups: \", len(FPdiv[FPdiv['d_accuracy'] > 0]))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(5).copy()\n",
    "print(\"Mean divergence top 5:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(10).copy()\n",
    "print(\"Mean divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(20).copy()\n",
    "print(\"Mean divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(50).copy()\n",
    "print(\"Mean divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv.copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faad084",
   "metadata": {},
   "source": [
    "### DivExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for XLSR300m rebalanced with DivExplorer\n",
    "config = 'xlsr_300_divexplorer'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "th_redundancy = None\n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)\n",
    "\n",
    "## Compute the mean divergence for XLSR300m\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_accuracy'] <= 0]))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(5).copy()\n",
    "print(\"Mean divergence top 5:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(10).copy()\n",
    "print(\"Mean divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(20).copy()\n",
    "print(\"Mean divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(50).copy()\n",
    "print(\"Mean divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "\n",
    "## Retrieve Most Positively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy) \n",
    "pr_top = FPdiv.head(n).copy()\n",
    "pr_top[\"support\"] = pr_top[\"support\"].round(2)\n",
    "pr_top[\"#errors\"] = pr_top[\"#errors\"].astype(int)\n",
    "pr_top[\"#corrects\"] = pr_top[\"#corrects\"].astype(int)\n",
    "pr_top[\"accuracy\"] = (pr_top[\"accuracy\"]*100).round(3)\n",
    "pr_top[\"d_accuracy\"] = (pr_top[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_top = pr_top[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_top['itemsets'] = pr_l_top['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_top)\n",
    "\n",
    "## Compute the mean divergence for XLSR300m\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)\n",
    "print(\"Total positive subgroups: \", len(FPdiv[FPdiv['d_accuracy'] > 0]))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(5).copy()\n",
    "print(\"Mean divergence top 5:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(10).copy()\n",
    "print(\"Mean divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(20).copy()\n",
    "print(\"Mean divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(50).copy()\n",
    "print(\"Mean divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv.copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375917df",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886111b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the divergence for XLSR300m rebalanced with Clustering\n",
    "config = 'xlsr_300_clustering'\n",
    "fp_divergence_i = fp_divergence_dict[config]\n",
    "th_redundancy = None\n",
    "\n",
    "n = 2\n",
    "\n",
    "## Retrieve Most Negatively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1] \n",
    "pr_bot = FPdiv.head(n).copy()\n",
    "pr_bot[\"support\"] = pr_bot[\"support\"].round(2)\n",
    "pr_bot[\"#errors\"] = pr_bot[\"#errors\"].astype(int)\n",
    "pr_bot[\"#corrects\"] = pr_bot[\"#corrects\"].astype(int)\n",
    "pr_bot[\"accuracy\"] = (pr_bot[\"accuracy\"]*100).round(3)\n",
    "pr_bot[\"d_accuracy\"] = (pr_bot[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_bot = pr_bot[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_bot['itemsets'] = pr_l_bot['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_bot)\n",
    "\n",
    "## Compute the mean divergence for XLSR300m\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)[::-1]\n",
    "print(\"Total negative subgroups: \", len(FPdiv[FPdiv['d_accuracy'] <= 0]))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(5).copy()\n",
    "print(\"Mean divergence top 5:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(10).copy()\n",
    "print(\"Mean divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(20).copy()\n",
    "print(\"Mean divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].head(50).copy()\n",
    "print(\"Mean divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] < 0].copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "\n",
    "## Retrieve Most Positively Divergent Itemsets \n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy) \n",
    "pr_top = FPdiv.head(n).copy()\n",
    "pr_top[\"support\"] = pr_top[\"support\"].round(2)\n",
    "pr_top[\"#errors\"] = pr_top[\"#errors\"].astype(int)\n",
    "pr_top[\"#corrects\"] = pr_top[\"#corrects\"].astype(int)\n",
    "pr_top[\"accuracy\"] = (pr_top[\"accuracy\"]*100).round(3)\n",
    "pr_top[\"d_accuracy\"] = (pr_top[\"d_accuracy\"]*100).round(3)\n",
    "## Choose columns for better visualization \n",
    "pr_l_top = pr_top[[ \"itemsets\", \"support\", \"accuracy\", \"d_accuracy\", \"t_value\"]].copy()\n",
    "pr_l_top['itemsets'] = pr_l_top['itemsets'].apply(lambda x: sortItemset(x, abbreviations))\n",
    "display(pr_l_top)\n",
    "\n",
    "## Compute the mean divergence for XLSR300m\n",
    "FPdiv = fp_divergence_i.getDivergence(th_redundancy=th_redundancy)\n",
    "print(\"Total positive subgroups: \", len(FPdiv[FPdiv['d_accuracy'] > 0]))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(5).copy()\n",
    "print(\"Mean divergence top 5:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(10).copy()\n",
    "print(\"Mean divergence top 10:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(20).copy()\n",
    "print(\"Mean divergence top 20:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].head(50).copy()\n",
    "print(\"Mean divergence top 50:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv[FPdiv['d_accuracy'] > 0].copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))\n",
    "pr = FPdiv.copy()\n",
    "print(\"Mean divergence all:\", round(100*pr['d_accuracy'].mean(), 3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DivExplorer_FSC_IC.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('speech': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "313.76837158203125px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "50f798c039f92e39594af06ec0119751541d975fa6ec3b2f5528645cd2e370ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
